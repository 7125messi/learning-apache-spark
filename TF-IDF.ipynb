{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "milton_paradise = gutenberg.raw('milton-paradise.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           sentences|\n",
      "+--------------------+\n",
      "|[Paradise Lost by...|\n",
      "|And chiefly thou,...|\n",
      "|Say first--for He...|\n",
      "|Who first seduced...|\n",
      "|Th' infernal Serp...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame({\n",
    "        'sentences': nltk.sent_tokenize(milton_paradise)\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.show(n=5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.gutenberg.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame({\n",
    "        'terms': [\n",
    "            ['spark', 'spark', 'spark', 'is', 'awesome', 'awesome'],\n",
    "            ['I', 'love', 'spark', 'very', 'very', 'much'],\n",
    "            ['everyone', 'should', 'use', 'spark']\n",
    "        ]\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|terms                                      |\n",
      "+-------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|\n",
      "|[I, love, spark, very, very, much]         |\n",
      "|[everyone, should, use, spark]             |\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashtf = HashingTF(numFeatures=pow(2, 4), inputCol='terms', \n",
    "                   outputCol='features(numFeatures), [index], [term frequency]')\n",
    "stages = [hashtf]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------------------------------------+\n",
      "|terms                                      |features(numFeatures), [index], [term frequency]|\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(16,[1,15],[4.0,2.0])                           |\n",
      "|[I, love, spark, very, very, much]         |(16,[0,1,2,8,12],[1.0,1.0,1.0,2.0,1.0])         |\n",
      "|[everyone, should, use, spark]             |(16,[1,9,13],[2.0,1.0,1.0])                     |\n",
      "+-------------------------------------------+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "countvectorizer = CountVectorizer(minTF=1.0, minDF=1.0, vocabSize=20, \n",
    "                                  inputCol='terms', outputCol='features(vocabSize), [index], [term frequency]')\n",
    "stages = [countvectorizer]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+----------------------------------------------+\n",
      "|terms                                      |features(vocabSize), [index], [term frequency]|\n",
      "+-------------------------------------------+----------------------------------------------+\n",
      "|[spark, spark, spark, is, awesome, awesome]|(10,[0,1,7],[3.0,2.0,1.0])                    |\n",
      "|[I, love, spark, very, very, much]         |(10,[0,2,5,8,9],[1.0,2.0,1.0,1.0,1.0])        |\n",
      "|[everyone, should, use, spark]             |(10,[0,3,4,6],[1.0,1.0,1.0,1.0])              |\n",
      "+-------------------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(df).transform(df).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   terms|\n",
      "+--------+\n",
      "|   spark|\n",
      "|   spark|\n",
      "|   spark|\n",
      "|      is|\n",
      "| awesome|\n",
      "| awesome|\n",
      "|       I|\n",
      "|    love|\n",
      "|   spark|\n",
      "|    very|\n",
      "|    very|\n",
      "|    much|\n",
      "|everyone|\n",
      "|  should|\n",
      "|     use|\n",
      "|   spark|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = countvectorizer.fit(df).vocabulary\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "df_vocab = df.select('terms').rdd.\\\n",
    "            flatMap(lambda x: x[0]).\\\n",
    "            toDF(schema=StringType()).toDF('terms')\n",
    "df_vocab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|frequency|      term|\n",
      "+---------+----------+\n",
      "|        5|   [spark]|\n",
      "|        2| [awesome]|\n",
      "|        2|    [very]|\n",
      "|        1|[everyone]|\n",
      "|        1|  [should]|\n",
      "|        1|    [much]|\n",
      "|        1|    [love]|\n",
      "|        1|      [is]|\n",
      "|        1|     [use]|\n",
      "|        1|       [I]|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_freq = df_vocab.rdd.countByValue()\n",
    "pdf = pd.DataFrame({\n",
    "        'term': vocab_freq.keys(),\n",
    "        'frequency': vocab_freq.values()\n",
    "    })\n",
    "tf = spark.createDataFrame(pdf).orderBy('frequency', ascending=False)\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__cmp__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__missing__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'default_factory',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'has_key',\n",
       " 'items',\n",
       " 'iteritems',\n",
       " 'iterkeys',\n",
       " 'itervalues',\n",
       " 'keys',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values',\n",
       " 'viewitems',\n",
       " 'viewkeys',\n",
       " 'viewvalues']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(vocab_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "stringindexer = StringIndexer(inputCol='terms', outputCol='StringIndexer(index)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   terms|StringIndexer(index)|\n",
      "+--------+--------------------+\n",
      "|   spark|                 0.0|\n",
      "| awesome|                 1.0|\n",
      "|    very|                 2.0|\n",
      "|      is|                 3.0|\n",
      "|everyone|                 4.0|\n",
      "|       I|                 5.0|\n",
      "|    love|                 6.0|\n",
      "|  should|                 7.0|\n",
      "|    much|                 8.0|\n",
      "|     use|                 9.0|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stringindexer.fit(df_vocab).transform(df_vocab).\\\n",
    "    distinct().\\\n",
    "    orderBy('StringIndexer(index)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   terms|\n",
      "+--------+\n",
      "|    love|\n",
      "|  should|\n",
      "|      is|\n",
      "|     use|\n",
      "|everyone|\n",
      "|   spark|\n",
      "|    much|\n",
      "|    very|\n",
      "| awesome|\n",
      "|       I|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vocab.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
