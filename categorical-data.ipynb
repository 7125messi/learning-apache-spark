{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkContext & SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(master = 'local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "          .appName(\"Python Spark SQL basic example\") \\\n",
    "          .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "          .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+\n",
      "| x1|    x2| x3| x4| y1| y2|\n",
      "+---+------+---+---+---+---+\n",
      "|  a| apple|  1|2.4|  1|yes|\n",
      "|  a|orange|  1|2.5|  0| no|\n",
      "|  b|orange|  2|3.5|  1| no|\n",
      "|  b|orange|  2|1.4|  0|yes|\n",
      "|  b| peach|  2|2.1|  0|yes|\n",
      "|  c| peach|  4|1.5|  1|yes|\n",
      "+---+------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pdf = pd.DataFrame({\n",
    "        'x1': ['a','a','b','b', 'b', 'c'],\n",
    "        'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach'],\n",
    "        'x3': [1, 1, 2, 2, 2, 4],\n",
    "        'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5],\n",
    "        'y1': [1, 0, 1, 0, 0, 1],\n",
    "        'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes']\n",
    "    })\n",
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StringIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StringIndexer` maps a string column to a index column that will be treated as a categorical column by spark. The indices start with 0 and are ordered by label frequencies. **If it is a numerical column, the column will first be casted to a string column and then indexed by  StringIndexer.**\n",
    "\n",
    "There are three steps to implement the StringIndexer\n",
    "\n",
    "1. Build the StringIndexer model: specify the input column and output column names.\n",
    "2. Learn the StringIndexer model: fit the model with your data.\n",
    "3. Execute the indexing: call the transform function to execute the indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+----------+\n",
      "| x1|    x2| x3| x4| y1| y2|indexed_x1|\n",
      "+---+------+---+---+---+---+----------+\n",
      "|  a| apple|  1|2.4|  1|yes|       1.0|\n",
      "|  a|orange|  1|2.5|  0| no|       1.0|\n",
      "|  b|orange|  2|3.5|  1| no|       0.0|\n",
      "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
      "|  b| peach|  2|2.1|  0|yes|       0.0|\n",
      "|  c| peach|  4|1.5|  1|yes|       2.0|\n",
      "+---+------+---+---+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# build indexer\n",
    "string_indexer = StringIndexer(inputCol='x1', outputCol='indexed_x1')\n",
    "\n",
    "# learn the model\n",
    "string_indexer_model = string_indexer.fit(df)\n",
    "\n",
    "# transform the data\n",
    "df_stringindexer = string_indexer_model.transform(df)\n",
    "\n",
    "# resulting df\n",
    "df_stringindexer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above, we can see that (a, b, c) in column x1 are converted to (1.0, 0.0, 2.0). They are ordered by their frequencies in column x1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneHotEncoder\n",
    "\n",
    "In R, categorical variables are automatically **dummy-coded** in data analysis, but pyspark doesn’t do this automatically. We will need to implement the OneHotEncoder to convert categorical variables to dummy variables.\n",
    "\n",
    "OneHotEncoder maps a column of **categorical indices** to a column of of **binary vectors**. Each index is converted to a **vector**. However, in spark the vector is represented by a **sparse vector**, becase sparse vector can save a lot of memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sparse vector** has three elements.\n",
    "\n",
    "* The first element is the length of regular vector.\n",
    "* The second element is a list of positions of non-zero elements in the regular vector.\n",
    "* The third element is a list of non-zero elements in the regular vector.\n",
    "For example with a regular vector [0, 2, 0, 1, 0], its sparse vector is [5, [1, 3], [2, 1]].\n",
    "\n",
    "**When using OneHotEncoder to dummy code a column of categorical indices, the last category is NOT included by default.**\n",
    "\n",
    "For example, with a categorical column of 3 indices, the following shows how the indices would be mapped to vectors:\n",
    "* 0.0 -> [1.0, 0.0, 0.0] -> [2, [0], [1.0]]\n",
    "* 1.0 -> [0.0, 1.0, 0.0] -> [2, [1], [1.0]]\n",
    "* 2.0 -> [0.0, 0.0, 1.0] -> [2, [], []] (**the last category is not included!**)\n",
    "\n",
    "**Note that in a dummy coding vector, only one element is non-zero, and it is always 1. Therefore, after applying the OneHotEncoder, the third element is always 1.0 in all vectors.**\n",
    "\n",
    "The process of using OneHotEncoder is different to using StingIndexer. There are only two steps.\n",
    "\n",
    "1. Build an indexer model\n",
    "2. Execute the indexing by calling transform\n",
    "\n",
    "Let’s see an implementing example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+----------+----------------+\n",
      "| x1|    x2| x3| x4| y1| y2|indexed_x1|onehotencoded_x1|\n",
      "+---+------+---+---+---+---+----------+----------------+\n",
      "|  a| apple|  1|2.4|  1|yes|       1.0|   (2,[1],[1.0])|\n",
      "|  a|orange|  1|2.5|  0| no|       1.0|   (2,[1],[1.0])|\n",
      "|  b|orange|  2|3.5|  1| no|       0.0|   (2,[0],[1.0])|\n",
      "|  b|orange|  2|1.4|  0|yes|       0.0|   (2,[0],[1.0])|\n",
      "|  b| peach|  2|2.1|  0|yes|       0.0|   (2,[0],[1.0])|\n",
      "|  c| peach|  4|1.5|  1|yes|       2.0|       (2,[],[])|\n",
      "+---+------+---+---+---+---+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# build indexer\n",
    "onehotencoder = OneHotEncoder(inputCol='indexed_x1', outputCol='onehotencoded_x1')\n",
    "\n",
    "# transform the data\n",
    "df_onehotencoder = onehotencoder.transform(df_stringindexer)\n",
    "\n",
    "# resulting df\n",
    "df_onehotencoder.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all categorical columns with Pipeline\n",
    "\n",
    "A **Pipeline** is a sequence of stages. A stage is an instance which has the property of either fit() or transform(). When fitting a Pipeline, the stages get executed in order. The example below shows how to use pipeline to process all categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+---+---+---+\n",
      "|onehotencoded_x1|onehotencoded_x2|onehotencoded_x3| x4| y1| y2|\n",
      "+----------------+----------------+----------------+---+---+---+\n",
      "|   (2,[1],[1.0])|       (2,[],[])|   (2,[1],[1.0])|2.4|  1|yes|\n",
      "|   (2,[1],[1.0])|   (2,[0],[1.0])|   (2,[1],[1.0])|2.5|  0| no|\n",
      "|   (2,[0],[1.0])|   (2,[0],[1.0])|   (2,[0],[1.0])|3.5|  1| no|\n",
      "|   (2,[0],[1.0])|   (2,[0],[1.0])|   (2,[0],[1.0])|1.4|  0|yes|\n",
      "|   (2,[0],[1.0])|   (2,[1],[1.0])|   (2,[0],[1.0])|2.1|  0|yes|\n",
      "+----------------+----------------+----------------+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = ['x1', 'x2', 'x3']\n",
    "\n",
    "##=== build stages ======\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehotencoded_' + c) for c in categorical_columns]\n",
    "all_stages = stringindexer_stages + onehotencoder_stages\n",
    "\n",
    "## build pipeline model\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=all_stages)\n",
    "\n",
    "## fit pipeline model\n",
    "pipeline_mode = pipeline.fit(df)\n",
    "\n",
    "## transform data\n",
    "df_coded = pipeline_mode.transform(df)\n",
    "\n",
    "## remove uncoded columns\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['x4', 'y1', 'y2']\n",
    "df_coded = df_coded.select(selected_columns)\n",
    "df_coded.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To fit a ML model in pyspark, we need to combine all feature columns into one single column of vectors: the featuresCol. The VectorAssembler can be used to combine multiple OneHotEncoder columns and other columns into one single column.\n",
    "\n",
    "The example below shows how to combine three OneHotEncoder columns and one numeric column into a featureCol column.\n",
    "\n",
    "VectorAssembler is similar to OneHotEncoder, there is not fit process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
